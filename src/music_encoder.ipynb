{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq wandb\n",
    "!export WANDB_API_KEY=\"e0aa96a51411bd4bc5a669e117bd1961648ff00e\"\n",
    "\n",
    "# Logeo a wandb.\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las librerías necesarias.\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as td\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import random_split\n",
    "# from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as tt\n",
    "from torchaudio.datasets import GTZAN\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizo GPU de estar disponible.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Seteo una semilla para replicabilidad.\n",
    "torch.manual_seed(181988)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(181988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos\n",
    "drive.mount('/content/drive')\n",
    "data_dir = '//content/drive/MyDrive/Materias/TD6 - Inteligencia Artificial/TPs/2023/TP4/genres_5sec/'\n",
    "list_files = os.listdir(data_dir)\n",
    "\n",
    "# Obtenemos las clases (géneros)\n",
    "classes=[]\n",
    "for file in list_files:\n",
    "  name='{}/{}'.format(data_dir,file)\n",
    "  if os.path.isdir(name):\n",
    "    classes.append(file)\n",
    "\n",
    "# Funciones auxiliares para la clase del dataset\n",
    "samplerate=22050\n",
    "\n",
    "# Para obtener generos\n",
    "def parse_genres(fname):\n",
    "    parts = fname.split('/')[-1].split('.')[0]\n",
    "    return parts #' '.join(parts[0])\n",
    "\n",
    "# Para definir transformación de audios (esto es data leakage hacerlo a todo? hay distintas formas: chequear clasificador.ipynb)\n",
    "def transform(audio):\n",
    "    return tt.Spectrogram(audio)\n",
    "\n",
    "# Definimos clase para obtener el dataset\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files =[]\n",
    "        for c in classes:\n",
    "          self.files = self.files + [fname for fname in os.listdir(os.path.join(root,c)) if fname.endswith('.wav')]\n",
    "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
    "        #self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        fname = self.files[i]\n",
    "\n",
    "        #img = self.transform(open_image(fpath))\n",
    "        genre = parse_genres(fname)\n",
    "        fpath = os.path.join(self.root,genre, fname)\n",
    "        class_idx = self.classes.index(genre)\n",
    "        audio = transform(torchaudio.load(fpath)[0])\n",
    "\n",
    "        return audio, class_idx\n",
    "\n",
    "dataset = MusicDataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido en training, validation, testing.\n",
    "val_size = 100\n",
    "test_size = 100\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dl = td.DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dl = td.DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_dl = td.DataLoader(test_ds,1, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        dense_layer_input_size = 64 * (input_size[1] // (2**3)) * (input_size[2] // (2**3)) # output channels de la ultima capa * reducción en 3 capas conv con stride=2\n",
    "        self.latent_space = nn.Linear(dense_layer_input_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        latent_rep = self.latent_space(x)\n",
    "        return latent_rep\n",
    "\n",
    "# Definición del decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 64, input_size[1] // (2**3), input_size[2] // (2**3))\n",
    "        reconstructed_seq = self.decoder(x)\n",
    "        return reconstructed_seq\n",
    "\n",
    "# Definición del Convolutional Autoencoder (CAE)\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_rep = self.encoder(x)\n",
    "        reconstructed_seq = self.decoder(latent_rep)\n",
    "        return reconstructed_seq, latent_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, label = dataset[0]\n",
    "\n",
    "# Hiperparámetros\n",
    "batch_size = 20\n",
    "learning_rate = 0.02\n",
    "epochs = 30\n",
    "type_input = 'spectrogram' # podría ser sino 'waveform' --> ajustar transform arriba\n",
    "input_size = audio.size() # en base al tamaño del tensor de audio\n",
    "latent_size = 32\n",
    "experiment_name = 'first_test'\n",
    "project_name = 'TP4'\n",
    "\n",
    "# Definición del modelo\n",
    "model = CAE(input_size, latent_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Configuración de wandb\n",
    "wandb.init(\n",
    "    project = project_name,\n",
    "    name = experiment_name,\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"type_input\": type_input,\n",
    "        \"latent_size\": latent_size\n",
    "    }\n",
    ")\n",
    "\n",
    "# Definición de función de perdida (MSE loss la que se suele usar para reconstrucción)\n",
    "criterion = nn.MSELoss()\n",
    "# Definición del optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Importante para ir liberando memoria ram\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() \n",
    "\n",
    "lowest_loss = -1\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    # Entrenamiento\n",
    "    model.train()\n",
    "    for batch in train_dl:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(inputs)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Importante para ir liberando memoria ram\n",
    "        del inputs \n",
    "        del loss\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    # Importante para ir liberando memoria ram\n",
    "    del inputs \n",
    "    del loss\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Para estadisticas de wandb\n",
    "    if val_losses[-1] < lowest_loss:\n",
    "        lowest_loss = val_losses[-1]\n",
    "        best_epoch = epoch\n",
    "\n",
    "    wandb.log({\"train_loss\": train_losses[-1], \"val_loss\": val_losses[-1]})\n",
    "\n",
    "    # Print progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {np.mean(train_losses):.4f}, Validation Loss: {np.mean(val_losses):.4f}')\n",
    "\n",
    "# Indico por consola cuando finalizó el entrenamiento\n",
    "print(f\"Entrenamiento finalizado, la loss más baja fue {str(lowest_loss)} la mejor epoch:{str(best_epoch)}\")\n",
    "\n",
    "# Guardo el modelo entrenado en disco\n",
    "PATH = './cae.pth'\n",
    "torch.save(best_model_state_dict, PATH)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
