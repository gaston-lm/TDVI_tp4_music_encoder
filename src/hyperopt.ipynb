{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las librerías necesarias.\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torchaudio\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as td\n",
    "import torchaudio.transforms as tt\n",
    "from torchaudio.datasets import GTZAN\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic1, ic2, ic3 = 16, 6, 8\n",
    "ks1, ks2, ks3 = 22, 5, 1\n",
    "s1, s2, s3 = 12, 2, 2\n",
    "p1, p2, p3 = 2, 1, 0\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, ic1, kernel_size=ks1, stride=s1, padding=p1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(ic1, ic2, kernel_size=ks2, stride=s2, padding=p2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(ic2, ic3, kernel_size=ks3, stride=s3, padding=p3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.latent_space = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        latent_rep = self.latent_space(x)\n",
    "        return latent_rep\n",
    "\n",
    "# Definición del decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.unflatten = nn.Unflatten(1, (8, 2297))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(ic3, ic2, kernel_size=ks3, stride=s3, padding=p3),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(ic2, ic1, kernel_size=ks2, stride=s2, padding=p2),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose1d(ic1, 1, kernel_size=ks1, stride=s1, padding=p1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.unflatten(x)\n",
    "        reconstructed_seq = self.decoder(x)\n",
    "        return reconstructed_seq\n",
    "\n",
    "# Definición del Convolutional Autoencoder (CAE)\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_rep = self.encoder(x)\n",
    "        reconstructed_seq = self.decoder(latent_rep)\n",
    "        return reconstructed_seq, latent_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizo GPU de estar disponible.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Seteo una semilla para replicabilidad.\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Conectamos la notebook a gdrive y seteamos data_dir con el path a los archivos\n",
    "drive.mount('/content/drive')\n",
    "data_dir = '//content/drive/MyDrive/UTDT/TD6/genres_5sec/'\n",
    "list_files = os.listdir(data_dir)\n",
    "\n",
    "# Obtenemos las clases (géneros)\n",
    "classes=[]\n",
    "for file in list_files:\n",
    "  name='{}/{}'.format(data_dir,file)\n",
    "  if os.path.isdir(name):\n",
    "    classes.append(file)\n",
    "\n",
    "# Funciones auxiliares para la clase del dataset\n",
    "samplerate=22050\n",
    "\n",
    "# Para obtener generos\n",
    "def parse_genres(fname):\n",
    "    parts = fname.split('/')[-1].split('.')[0]\n",
    "    return parts #' '.join(parts[0])\n",
    "\n",
    "# Para definir transformación de audios (esto es data leakage hacerlo a todo? hay distintas formas: chequear clasificador.ipynb)\n",
    "def transform(audio):\n",
    "    return tt.Spectrogram()(audio)\n",
    "\n",
    "# Definimos clase para obtener el dataset\n",
    "class MusicDataset():\n",
    "    def __init__(self, root):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files =[]\n",
    "        for c in classes:\n",
    "          self.files = self.files + [fname for fname in os.listdir(os.path.join(root,c)) if fname.endswith('.wav')]\n",
    "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
    "        self.transform = tt.Spectrogram()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        fname = self.files[i]\n",
    "        genre = parse_genres(fname)\n",
    "        fpath = os.path.join(self.root,genre, fname)\n",
    "        class_idx = self.classes.index(genre)\n",
    "        audio = torchaudio.load(fpath)[0]\n",
    "        spectrogram = self.transform(audio)\n",
    "\n",
    "        return audio, spectrogram, class_idx\n",
    "\n",
    "dataset = MusicDataset(data_dir)\n",
    "\n",
    "# Divido en training, validation, testing.\n",
    "val_size = 100\n",
    "test_size = 100\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_ds, val_ds, test_ds = td.random_split(dataset, [train_size, val_size, test_size], generator)\n",
    "\n",
    "train_dl = td.DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_dl = td.DataLoader(val_ds, batch_size, num_workers=2, pin_memory=True)\n",
    "test_dl = td.DataLoader(test_ds,1, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cae(params):\n",
    "\n",
    "  print(f\"Using lr = {params['learning_rate']}\")\n",
    "  \n",
    "  # Inicializo model\n",
    "  model = CAE()\n",
    "  model.to(device)\n",
    "\n",
    "  # Definición de función de perdida (MSE loss la que se suele usar para reconstrucción)\n",
    "  criterion = RMSELoss()\n",
    "  # Definición del optimizador\n",
    "  optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "  # Importante para ir liberando memoria ram\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "\n",
    "  lowest_loss = 1000\n",
    "  best_epoch = -1\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      train_losses = []\n",
    "      # Entrenamiento\n",
    "      model.train()\n",
    "      for batch in train_dl:\n",
    "          optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "          inputs, _, _ = batch\n",
    "          inputs = inputs.to(device)\n",
    "          # Forward pass\n",
    "          outputs, _ = model(inputs)\n",
    "\n",
    "          # Compute reconstruction loss\n",
    "          loss = criterion(outputs, inputs)\n",
    "\n",
    "          # Backward pass and optimization\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          train_losses.append(loss.item())\n",
    "\n",
    "          # Importante para ir liberando memoria ram\n",
    "          del inputs\n",
    "          del loss\n",
    "          del outputs\n",
    "          torch.cuda.empty_cache()\n",
    "          gc.collect()\n",
    "\n",
    "      # Validation\n",
    "      model.eval()\n",
    "      val_losses = []\n",
    "      with torch.no_grad():\n",
    "          for batch in valid_dl:\n",
    "              inputs, _, _ = batch\n",
    "              inputs = inputs.to(device)\n",
    "              outputs, _ = model(inputs)\n",
    "              loss = criterion(outputs, inputs)\n",
    "              val_losses.append(loss.item())\n",
    "\n",
    "      # Importante para ir liberando memoria ram\n",
    "      del inputs\n",
    "      del loss\n",
    "      del outputs\n",
    "      torch.cuda.empty_cache()\n",
    "      gc.collect()\n",
    "\n",
    "      # Para estadisticas de wandb\n",
    "      if val_losses[-1] < lowest_loss:\n",
    "          lowest_loss = val_losses[-1]\n",
    "          best_epoch = epoch\n",
    "          best_model_state_dict = model.state_dict()\n",
    "\n",
    "      # Print progress\n",
    "      print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {np.mean(train_losses):.4f}, Validation Loss: {np.mean(val_losses):.4f}')\n",
    "\n",
    "  # Indico por consola cuando finalizó el entrenamiento\n",
    "  print(f\"Entrenamiento finalizado, la loss más baja fue {str(lowest_loss)} la mejor epoch:{str(best_epoch)}\")\n",
    "\n",
    "  return lowest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1)\n",
    "}\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "best = fmin(fn=train_cae, space=space, algo=tpe.suggest, max_evals=8)\n",
    "print(\"Best learning rate:\", best['learning_rate'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
